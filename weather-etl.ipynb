{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import requests as rq\n",
    "import tarfile\n",
    "import sqlalchemy as sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition cell\n",
    "# Download and extract station records that might be lower 48 from a given year\n",
    "\n",
    "def download_extract_na_stations(year):\n",
    "    # Once year 2000 data is extracted, terminate the extraction-cleaning loop\n",
    "    if year >= 2001:\n",
    "        print(\"Complete\")\n",
    "        return\n",
    "    \n",
    "    ext_dir = '../dataholder/'\n",
    "    download_path = '../tarball/'\n",
    "\n",
    "    # Make sure dirs are empty\n",
    "    fl = os.listdir(ext_dir)\n",
    "    fl2 = os.listdir(download_path)\n",
    "    for fn in fl:\n",
    "        fp = os.path.join(ext_dir, fn)\n",
    "        os.remove(fp)\n",
    "    for fn in fl2:\n",
    "        fp = os.path.join(download_path, fn)\n",
    "        os.remove(fp)\n",
    "\n",
    "    url = f\"https://www.ncei.noaa.gov/data/global-summary-of-the-day/archive/{year}.tar.gz\"\n",
    "\n",
    "    # Create the download and extraction directories if they don't exist\n",
    "    os.makedirs(download_path, exist_ok=True)\n",
    "    os.makedirs(ext_dir, exist_ok=True)\n",
    "\n",
    "    # Download the tarball\n",
    "    print(f\"Getting tarball from {url}\")\n",
    "    response = rq.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        tarball_path = os.path.join(download_path, f'{year}.tar.gz')\n",
    "        with open(tarball_path, 'wb') as tarball_file:\n",
    "            tarball_file.write(response.content)\n",
    "\n",
    "        # Extract only files that might be lower 48 stations\n",
    "        with tarfile.open(tarball_path, 'r:gz') as tar:\n",
    "            members = tar.getmembers()\n",
    "            filtered_members = [member for member in members if member.name.startswith('7') or member.name.startswith('69') or member.name.startswith('99')]\n",
    "            tar.extractall(path=ext_dir, members=filtered_members)\n",
    "            print(f\"Stations extracted for year {year}: {len(filtered_members)}\")\n",
    "\n",
    "        print(f\"{year} extraction complete.\")\n",
    "        # Remove the downloaded tarball after extraction\n",
    "        os.remove(tarball_path)  \n",
    "    else:\n",
    "        print(f\"Failed to download the tarball. Status code: {response.status_code}\")\n",
    "    # Run the cleaning function for the contents of dataholder\n",
    "    clean2(year)\n",
    "\n",
    "    # Further location paredown and useful column isolation\n",
    "def clean2(year):\n",
    "    global counter\n",
    "    file_path = '../dataholder/*.csv'\n",
    "    dfs = []\n",
    "    station_list = []\n",
    "\n",
    "    for file in glob.glob(file_path):\n",
    "        # print(f\"Reading {file}\")\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            # Skip files with missing location data\n",
    "            if df['LATITUDE'].isnull().any() or df['LONGITUDE'].isnull().any() or df['NAME'].isnull().any():\n",
    "                print(f\"Skipping {file}: Missing location data\")\n",
    "                continue\n",
    "\n",
    "            # Skip files not in US per name\n",
    "            if not df.loc[0]['NAME'].endswith(\"US\"):\n",
    "                print(f\"Skipping {file}: Name not US\")\n",
    "                continue\n",
    "\n",
    "            # Skip files whose coordinates rule out lower 48\n",
    "            lat = df.loc[0]['LATITUDE']\n",
    "            long = df.loc[0]['LONGITUDE']\n",
    "            if lat <= 24 or lat >= 50 or long <= -125:\n",
    "                print(f\"Skipping {file}: Coordinates rejected\")\n",
    "                continue\n",
    "            \n",
    "            station_id = df.loc[0][['STATION', 'NAME']]\n",
    "            station_list.append(station_id)\n",
    "            # # Use rg to catch any remaining non-US stations\n",
    "            # coords = (lat, long)\n",
    "            # place = rg.search(coords)\n",
    "            # if place[0]['cc'] != 'US':\n",
    "            #     print(f\"{file} not US per rg\")\n",
    "            #     continue\n",
    "\n",
    "            # Capture interesting columns and concat into new csv\n",
    "            our_data = df[['DATE', 'LATITUDE', 'LONGITUDE', 'TEMP', 'MAX', 'MIN', 'PRCP', 'NAME']]\n",
    "            dfs.append(our_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file}: {e}\")\n",
    "\n",
    "    # Merge output into one csv\n",
    "    dfout = pd.concat(dfs, ignore_index=True)\n",
    "    dfout.to_csv(f'./cleaned_annual_csvs/df{year}.csv')  \n",
    "    stations = pd.concat(station_list, ignore_index=True)\n",
    "    stations.to_csv(f'./annual_stations/stations {year}.csv')\n",
    "    \n",
    "    # Empty extraction directory\n",
    "    ext_dir = '../dataholder/'\n",
    "    fl = os.listdir(ext_dir)\n",
    "    for fn in fl:\n",
    "        fp = os.path.join(ext_dir, fn)\n",
    "        os.remove(fp)\n",
    "\n",
    "    # Set counter to next year, call download function again\n",
    "    counter += 1\n",
    "    download_extract_na_stations(counter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monthly aggregation, averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition cell\n",
    "# Read each annual data csv generated by download & extract function and generate desired metrics for every month in the range of years\n",
    "\n",
    "def aggregate_average():\n",
    "    final = pd.DataFrame(columns = ['Month', 'mean_temp-MEAN', 'mean_temp-MEDIAN', 'mean_temp-VARIANCE', 'mean_temp-MIN', 'mean_temp-MAX', 'min_temp-MEAN', 'min_temp-MEDIAN', 'min_temp-VARIANCE', 'min_temp-MIN', 'min_temp-MAX', 'max_temp-MEAN', 'max_temp-MEDIAN', 'max_temp-VARIANCE', 'max_temp-MIN', 'max_temp-MAX', 'precip-MEAN', 'precip-MEDIAN', 'precip-VARIANCE', 'precip-MIN', 'precip-MAX'])\n",
    "    \n",
    "    for year in range(1950, 2001, 1):\n",
    "        try:\n",
    "            df = pd.read_csv(f'./cleaned_annual_csvs/df{year}.csv')\n",
    "        except:\n",
    "            print(f\"File df{year}.csv not found\")\n",
    "        else:\n",
    "            df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "\n",
    "            months = [df[df['DATE'].dt.month == i] for i in range(1, 13)]\n",
    "            for i, month_df in enumerate(months, start=1):\n",
    "                nn1 = month_df[month_df['TEMP'] != 9999.9] # Generate a dataframe missing all rows where 'TEMP' reading is null; use this for 'TEMP' calculations\n",
    "                mean_temp_mean = round(nn1['TEMP'].mean(), 2)\n",
    "                mean_temp_median = round(nn1['TEMP'].median(), 2)\n",
    "                mean_temp_variance = round(nn1['TEMP'].var(), 2)\n",
    "                mean_temp_min = nn1['TEMP'].min()\n",
    "                mean_temp_max = nn1['TEMP'].max()\n",
    "                \n",
    "                nn2 = month_df[month_df['MIN'] != 9999.9] # missing all rows where 'MIN' is null\n",
    "                min_temp_mean = round(nn2['MIN'].mean(), 2)\n",
    "                min_temp_median = round(nn2['MIN'].median(), 2)\n",
    "                min_temp_variance = round(nn2['MIN'].var(), 2)\n",
    "                min_temp_min = nn2['MIN'].min()\n",
    "                min_temp_max = nn2['MIN'].max()\n",
    "\n",
    "                nn3 = month_df[month_df['MAX'] != 9999.9] # missing all rows where 'MAX' is null\n",
    "                max_temp_mean = round(nn3['MAX'].mean(), 2)\n",
    "                max_temp_median = round(nn3['MAX'].median(), 2)\n",
    "                max_temp_variance = round(nn3['MAX'].var(), 2)\n",
    "                max_temp_min = nn3['MAX'].min()\n",
    "                max_temp_max = nn3['MAX'].max()\n",
    "\n",
    "                nn4 = month_df[month_df['PRCP'] != 99.99] # missing all rows where 'PRCP' is null. Per documentation many stations use this to record no rainfall. Replacing null with 0 may be preferable here?\n",
    "                prcp_mean = round(nn4['PRCP'].mean(), 2)\n",
    "                prcp_median = round(nn4['PRCP'].median(), 2)\n",
    "                prcp_variance = round(nn4['PRCP'].var(), 2)\n",
    "                prcp_min = nn4['PRCP'].min()\n",
    "                prcp_max = nn4['PRCP'].max()\n",
    "\n",
    "                row_data = [\n",
    "                    f'{i}-{year}', \n",
    "                    mean_temp_mean, mean_temp_median, mean_temp_variance, mean_temp_min, mean_temp_max, \n",
    "                    min_temp_mean, min_temp_median, min_temp_variance, min_temp_min, min_temp_max,\n",
    "                    max_temp_mean, max_temp_median, max_temp_variance, max_temp_min, max_temp_max,\n",
    "                    prcp_mean, prcp_median, prcp_variance, prcp_min, prcp_max\n",
    "                ]\n",
    "\n",
    "                print(row_data)\n",
    "                final.loc[len(final)] = row_data\n",
    "    \n",
    "    # Clean up intermediate CSVs\n",
    "    deletion_dir = './cleaned_annual_csvs/'\n",
    "\n",
    "    fl = os.listdir(deletion_dir)\n",
    "    for fn in fl:\n",
    "        fp = os.path.join(deletion_dir, fn)\n",
    "        os.remove(fp)\n",
    "\n",
    "    final.to_csv('final.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload to SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition cell\n",
    "def generate_sql():\n",
    "    do_sql = sql.create_engine('postgresql://postgres:password@localhost:5432/postgres')\n",
    "    try:\n",
    "        df = pd.read_csv('final.csv')\n",
    "    except:\n",
    "        print('generate_sql: file final.csv not found')\n",
    "    else:\n",
    "        with do_sql.begin() as connection:\n",
    "            df.to_sql(\"month-aggregated-weather-stats-1950-2000\", con=connection, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execution cell\n",
    "counter = 1950\n",
    "download_extract_na_stations(counter)\n",
    "aggregate_average()\n",
    "generate_sql()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup cell\n",
    "ext_dir = '../dataholder/'\n",
    "fl = os.listdir(ext_dir)\n",
    "for fn in fl:\n",
    "    fp = os.path.join(ext_dir, fn)\n",
    "    os.remove(fp)\n",
    "# ast = './annual_stations/'\n",
    "# fl = os.listdir(ast)\n",
    "# for fn in fl:\n",
    "#     fp = os.path.join(ast, fn)\n",
    "#     os.remove(fp)\n",
    "deletion_dir = './cleaned_annual_csvs/'\n",
    "fl = os.listdir(deletion_dir)\n",
    "for fn in fl:\n",
    "    fp = os.path.join(deletion_dir, fn)\n",
    "    os.remove(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge whole year into one csv\n",
    "    # joined_files = os.path.join('../dataholder/*.csv')\n",
    "    # joined_list = glob.glob(joined_files)\n",
    "    # year_all_data = pd.concat(map(pd.read_csv, joined_list), ignore_index=True)\n",
    "    # print(year_all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make a testing subset\n",
    "    # def make_subset_20(src_dir, dest_dir):\n",
    "    #     os.makedirs(dest_dir, exist_ok = True)\n",
    "    #     files = os.listdir(src_dir)\n",
    "\n",
    "    #     for i, file_name in enumerate(files):\n",
    "    #         if i % 20 == 0:\n",
    "    #             src_path = os.path.join(src_dir, file_name)\n",
    "    #             dest_path = os.path.join(dest_dir, file_name)\n",
    "\n",
    "    #             shutil.copy2(src_path, dest_path)\n",
    "\n",
    "    # if __name__ == \"__main__\":\n",
    "    #     source = '../dataholder'\n",
    "    #     destination = '../dataholder/test_subset'\n",
    "    #     make_subset_20(source, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test stuff\n",
    "# df = pd.read_csv('../dataholder/70265099999.csv')\n",
    "# lat = df.loc[0]['LATITUDE']\n",
    "# long = df.loc[0]['LONGITUDE']\n",
    "# coords = (lat, long)\n",
    "# r = pd.DataFrame(coords)\n",
    "# z = 1777\n",
    "# place = rg.search(coords)\n",
    "\n",
    "# print(df.loc[0]['NAME'].endswith(\"US\"))\n",
    "# r.to_csv(f\"df{z}.csv\")\n",
    "# print(loc[0]['admin1'] == 'Alaska')\n",
    "# our_data = df[['DATE', 'LATITUDE', 'LONGITUDE', 'TEMP', 'MAX', 'MIN', 'PRCP']]\n",
    "# our_data\n",
    "\n",
    "# file_path = './cleaned_annual_csvs/*.csv'\n",
    "    \n",
    "# for file in glob.glob(file_path):\n",
    "#     print(file[-8:-4])\n",
    "\n",
    "# url = \"https://www.ncei.noaa.gov/data/global-summary-of-the-day/archive/1950.tar.gz\"\n",
    "# download_path = '../tarball/'\n",
    "\n",
    "# print(f\"Getting tarball from {url}\")\n",
    "# response = rq.get(url)\n",
    "# if response.status_code == 200:\n",
    "#     tarball_path = os.path.join(download_path, f'1950.tar.gz')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
